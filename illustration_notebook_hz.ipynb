{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_random_seed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore_hz import create_dataset, data_loader, Ngram, MLP, generate, evaluate\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, chars, max_word_length = create_dataset('names.txt')\n",
    "_, train_loader = data_loader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system inits\n",
    "set_random_seed(714)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2334, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2549, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0690, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1655, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0831, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6835, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5344, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4205, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1563, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6368, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3309, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1750, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1615, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3746, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9560, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2677, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9588, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8597, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2713, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0298, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7029, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0593, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1715, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2518, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3750, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0603, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3307, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1684, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3186, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1517, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2058, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1666, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4883, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1510, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1192, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0630, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9847, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3187, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5033, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1022, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implement a train function (without regularization, to-do: E03, E05)\n",
    "epoch = 50\n",
    "model = MLP(vocal_size=train_dataset.get_vocab_size(), markov_order=3, emb_dim=32, hid_dim=32)\n",
    "#model = Ngram(vocal_size=train_dataset.get_vocab_size(), markov_order=3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01) # weight decay is equivalent to adding L2 regularization for SGD, similar for momentum algorithm and ADAM \n",
    "\n",
    "for i in range(epoch):\n",
    "    for _, (xspt, yspt) in enumerate(train_loader):\n",
    "        logits, loss = model(xspt, yspt)\n",
    "        #print(loss)\n",
    "\n",
    "        # calculate the gradient, update the weights\n",
    "        model.zero_grad(set_to_none=True) # model.zero_grad = optimizer.zero_grad if all model parameters are in one optimizer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.226015567779541\n",
      "2.2519023418426514\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, val_dataset))\n",
    "print(evaluate(model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liha\n",
      "cavelegulakie\n",
      "aalen\n",
      "redyn\n",
      "aansh\n",
      "yoit\n",
      "eylarta\n",
      "aania\n",
      "igensiban\n",
      "lan\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(train_dataset.decode(generate(model, train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
